{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shlex\n",
    "\n",
    "from pyspark.sql import Row\n",
    "log_file = sc.textFile(\"../data/log_file.txt\")\n",
    "\n",
    "log_file.takeSample(True, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splits = log_file.map(lambda row: shlex.split(row))\n",
    "splits.cache()\n",
    "splits.takeSample(True, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_schema(row):\n",
    "  ip = row[0]\n",
    "  date = row[1].replace('[', '').replace(']', '')\n",
    "  tokens = row[2].split(' ')\n",
    "  protocol = tokens[0]\n",
    "  url = tokens[1].split('?')[0]\n",
    "  status = row[3]\n",
    "  time = None if row[4] == '-' else int(row[4]) \n",
    "  return Row(ip=ip, date=date, protocol=protocol, url=url, status=status, time=time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "schema_DF = splits.map(create_schema).toDF()\n",
    "schema_DF.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlCtx.registerDataFrameAsTable(schema_DF, 'logs')\n",
    "sample = sqlCtx.sql('SELECT * FROM logs LIMIT 10').collect()\n",
    "for row in sample:\n",
    "    print row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find 10 most popular url's\n",
    "url_access = sqlCtx.sql('''SELECT url, count(*) as counts FROM logs GROUP BY url\n",
    "  ORDER BY counts DESC LIMIT 10''').collect()\n",
    "for row in url_access:\n",
    "    print row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 10 highest traffic sources\n",
    "ip_access = sqlCtx.sql(\"SELECT ip, count(*) as counts FROM logs GROUP BY ip ORDER BY counts DESC LIMIT 10\").collect()\n",
    "for row in ip_access:\n",
    "    print row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# same operation without sparkSQL\n",
    "ip_access_direct = schema_dicts.map(lambda row: (row.ip, 1)).reduceByKey(lambda a,b: a+b).map(\n",
    " lambda r: (r[1], r[0])).sortByKey(ascending=False).map(lambda r: (r[1], r[0])).collect()\n",
    "for row in ip_access[:10]:\n",
    "    print row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "step1 = schema_dicts.map(lambda row: (row.ip, 1))\n",
    "step1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "step2 = step1.reduceByKey(lambda a,b: a+b)\n",
    "step2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "step3 = step2.map(lambda r: (r[1], r[0]))\n",
    "step3.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "step4 = step3.sortByKey(ascending=False)\n",
    "step4.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "step5 = step4.map(lambda r: (r[1], r[0]))\n",
    "step5.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "pq_path = '../data/log_{}.pq'.format(int(time()))\n",
    "log_schema.saveAsParquetFile(pq_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parquetFile = sqlCtx.parquetFile(pq_path)\n",
    "parquetFile.registerTempTable('parquetTable')\n",
    "\n",
    "ip_access = sqlCtx.sql(\"SELECT ip, count(*) as counts FROM parquetTable GROUP BY ip ORDER BY counts DESC LIMIT 10\").collect()\n",
    "for row in ip_access:\n",
    "    print row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
